% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\author{}
\date{\vspace{-2.5em}}

\begin{document}

A Dive Into Danish NLP

Exploring the Strengths and Weaknesses of Danish Language Models

Cognitive Science // Bachelor Thesis 2023

JÃ¸rgen HÃ¸jlund Wibe \& Niels Aalund Krogsgaard

Table of Contents

About the project

Structure of the readme file

Section 1: Topic Modelling

Setup

Conclusion

Section 2: Fine-Tuning Models

Setup

Content of each notebook

Conclusion

Section 3: Evaluation Method

Setup

Content of markdown file

Conclusion

Contact

Acknowledgements

\hypertarget{about-the-project}{%
\subsection{About the project}\label{about-the-project}}

This repository contains all the notebooks and files necessarry to
reproduce the results found in the paper \emph{`A Deep Dive into Danish
NLP'}.

The project consists of three phases: In the first phase, we test and
compare four different topic modelling methods to find the one that
produces the most human interpretable topics from a Danish Twitter
dataset. In the second phase, we fine-tune 11 Danish language models on
a multilabel classification task on the same Twitter dataset. In the
last phase, we outline a new method for doing model error analysis on
the subgroups made by the best topic model.

\hypertarget{structure-of-the-readme-file}{%
\subsubsection{Structure of the readme
file}\label{structure-of-the-readme-file}}

This readme file will be structured according to the above outlined
phases. Thus, \textbf{section 1} contains details on which topic models
were compared and provides links to notebooks which will enable a
replication of the results presented in the bachelor thesis. Similarly,
\textbf{section 2} contains details on which models were trained along
with links to individual notebooks containing the scripts used for
fine-tuning. The last \textbf{section (3)} contains a reproducible
R-markdown script outlining how the sub-group error analysis was
performed.

\hypertarget{requirements-to-reproduce-results}{%
\subsection{Requirements to reproduce
results}\label{requirements-to-reproduce-results}}

\begin{itemize}
\tightlist
\item
  A machine with a GPU (we used Google Colab: a virtual notebook
  environment that executes code on virtual machines with GPU)
\item
  Python 3.6 or higher
\item
  An R-markdown capable IDE
\end{itemize}

Section 1

Topic Modelling

\hypertarget{set-up}{%
\subsection{Set up}\label{set-up}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone this repository:
\end{enumerate}

\begin{verbatim}
git clone https://github.com/jorgenhw/bachelor_thesis
cd bachelor_thesis/topic_modelling
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Open notebook of interest
\end{enumerate}

Open the notebook with the name of the topic modelling method you want
to examine e.g.~\texttt{LDA\_-\_generating\_topics.ipynb} either through
your own IDE or through Google Colab (links are provided in the table
below).

The below table outlines which notebooks contains which methods.

\begin{longtable}[]{@{}lll@{}}
\toprule
Method & Filename & Colab link\tabularnewline
\midrule
\endhead
Non-Negative Matrix Factorization (NMF) &
\texttt{NMF\_-\_generating\_topics.ipynb} &\tabularnewline
Latent Dirichlet Allocation & \texttt{LDA\_-\_generating\_topics.ipynb}
&
\href{https://colab.research.google.com/drive/1_3Gw_y6jSUMaid17sgceX2q-nvCrN6Yk}{\includegraphics{https://colab.research.google.com/assets/colab-badge.svg}}\tabularnewline
TweeTopic & \texttt{GSDMM\_-\_generating\_topics.ipynb} &
\href{https://colab.research.google.com/drive/1IfgMAcWC6MxaaFp4plrix75dEKWzfsRP}{\includegraphics{https://colab.research.google.com/assets/colab-badge.svg}}\tabularnewline
BERTopic & \texttt{BERTopic\_-\_generating\_topics.ipynb} &
\href{https://colab.research.google.com/drive/1Lxfx4Ke2TGp-UoxqOlMTgLkOQH3RRdVL}{\includegraphics{https://colab.research.google.com/assets/colab-badge.svg}}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

In conclusion, the experiment we conducted to compare the performance of
four topic modelling methods - NMF, LDA, BERTopic, and TweeTopic -
showed that BERTopic was the best performer, meaning it was able to
create the most human interpretable subgroups in the data. This was
demonstrated through the evaluation method of qualitative assessment.

Section 2

Model Fine-tuning

\hypertarget{set-up-1}{%
\subsection{ðŸ”§ Set up}\label{set-up-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone this repository:
\end{enumerate}

\begin{verbatim}
git clone https://github.com/jorgenhw/bachelor_thesis
cd bachelor_thesis/model_fine_tuning
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Open notebook Open the notebook
  \texttt{fine\_tuning\_reproducible.ipynb} either through your own IDE
  or through Google Colab (link is provided in the table below).
\end{enumerate}

All 11 models examined are fine-tuned using the exact same script. The
below script trains a RoBERTa model so one can run the script right off
the batch, however, to train another model with the script, simply
replace the model name.

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & Filename & Colab link\tabularnewline
\midrule
\endhead
RoBERTa & \texttt{fine\_tuning\_reproducible.ipynb} &
\href{https://colab.research.google.com/drive/1I4GyXlebR7q1nbQ6uOR92cFyqCdDTgW2\#scrollTo=sR5zEsTidz5e}{\includegraphics{https://colab.research.google.com/assets/colab-badge.svg}}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{content-of-the-notebook}{%
\subsection{Content of the notebook}\label{content-of-the-notebook}}

The notebook consist of five steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialization of GPU, installation of necesarry packages and setup of
  WANDB
\item
  Importing libraries, data, and the language model
\item
  Data preprocessing
\item
  Hyperparamter tuning
\item
  Fine-tuning
\end{enumerate}

\hypertarget{conclusion-1}{%
\subsection{Conclusion}\label{conclusion-1}}

In this project, we have successfully finetuned 11 Danish language
models to demonstrate how a more fine-grained sub group error analysis
metric can reveal new insights into language models. Suggestions for
further research can be found in the paper \emph{`A Deep Dive Into
Danish NLP'} (link provided in the top of this readme).

This project is part of a bachelor thesis in Cognitive Science at Aarhus
University, Denmark, 2023.

Section 3

Evaluation Method

\hypertarget{set-up-2}{%
\subsection{ðŸ”§ Set up}\label{set-up-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone this repository:
\end{enumerate}

\begin{verbatim}
git clone https://github.com/jorgenhw/bachelor_thesis
cd bachelor_thesis/analysis_R
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Open markdown file
\end{enumerate}

\hypertarget{content-of-markdown-file}{%
\subsection{Content of markdown file}\label{content-of-markdown-file}}

The markdown file consist of three steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loading data
\item
  Calculating RTAC scores
\item
  Model Comparison with Bayesian Modelling

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Mono- vs.~Multilingual Language Models
  \item
    Base vs.~Large Language Models
  \item
    Language Models of different BERT-style architectures
  \end{enumerate}
\end{enumerate}

\hypertarget{conclusion-2}{%
\subsection{Conclusion}\label{conclusion-2}}

In this markdown file we demonstrate how to conduct a subgroup error
analysis on the performance of the fine-tuned models (section 2) in the
the topics made by BERTopic in section 1.

Instead of arriving at the trivial conclusion that larger models also
have the highest accuracy in the sub-groups, we instead calculate the
difference between each sub-group accuracy and the overall accuracy of a
given language model. This is done through leave-one-group-out mean
calculation to reduce the data-leakage between accuracy scores, since we
are interested in the difference between a sub-group and all other
groups that are not that sub-group. We call the resulting values a
Relative Topic Accuracy Correction (RTAC).

ðŸ’¬ Contact ðŸ’¬

Feel free to contact the authors,
\href{https://github.com/jorgenhw}{JÃ¸rgen HÃ¸jlund Wibe} or
\href{https://github.com/nielsaak}{Niels Aalund Krogsgaard} for any
questions regarding the project. You may do so through our emails
(\href{mailto:201807750@post.au.dk}{JÃ¸rgen},
\href{mailto:202008114@post.au.dk}{Niels})

Acknowledgements

We would like to express our sincere gratitude to Google Colab and
Hugging Face for their invaluable contributions to the field of machine
learning and natural language processing.

Google Colab has provided us with a powerful platform for conducting
research and development, allowing us to access state-of-the-art
resources and technologies without the need for expensive hardware or
software. Its intuitive interface and seamless integration with Google
Drive have made it an essential tool for collaborating together and
sharing our findings.

Hugging Face, on the other hand, has revolutionized the way we work with
transformer-based models, providing us with a vast library of
pre-trained models and a user-friendly API that allows us to easily
fine-tune and deploy them for various tasks. Its commitment to open
source and constantly updating its offerings have made it a go-to
resource for researchers and practitioners alike.

\end{document}
