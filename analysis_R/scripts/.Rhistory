unique(Text)
test <- topic_clas %>%
group_by(run) %>%
unique(topic_clas$Text)
test <- topic_clas %>%
group_by(run) %>%
distinct(Text)
test <- topic_clas %>%
group_by(run) %>%
distinct(Text)
View(topic_clas)
topics1 <- topics %>%
rename(Text = original_tweet)
View(classification_report_jondf)
View(topics)
unique(topics$original_tweet)
length(unique(topics$original_tweet))
test <- filter(unique(topics$original_tweet))
test <- topics %>%
filter(unique(topics$original_tweet))
test <- topics %>%
filter(distinct(topics$original_tweet))
test <- topics %>%
filter(unique(topics$original_tweet))
test <- topics %>%
dplyr::filter(unique(topics$original_tweet))
topics <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/BerTopic/sub_groups_07112022.csv")
topic_clas <- merge(classification_report_jondf, topics, by = "Text", all.x = T, all.y = F)
topics <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/BerTopic/sub_groups_07112022.csv")
topics1 <- topics %>%
rename(Text = original_tweet)
topic_clas <- merge(classification_report_jondf, topics, by = "Text", all.x = T, all.y = F)
names(classification_report_jondf)
names(topics)
topic_clas <- merge(classification_report_jondf, topics1, by = "Text", all.x = T, all.y = F)
topics_clas2 <- topics_clas %>%
select(Text, `Predicted Label`, `True Labels`, run, new_topic)
topics_clas2 <- topic_clas %>%
select(Text, `Predicted Label`, `True Labels`, run, new_topic)
names(topic_clas)
topics_clas2 <- topic_clas %>%
select(Text, `Predicted Labels`, `True Labels`, run, new_topic)
View(topics_clas2)
topics2 <- topics1 %>%
distinct(Text, keep_all = TRUE)
View(topics2)
topics2 <- topics1 %>%
distinct(Text, .keep_all = TRUE)
View(topics2)
topic_clas <- merge(classification_report_jondf, topics2, by = "Text", all.x = T, all.y = F)
topics_clas2 <- topic_clas %>%
select(Text, `Predicted Labels`, `True Labels`, run, new_topic)
View(topics_clas2)
View(classification_report_jondf)
View(topic_clas)
topics_clas2 <- topic_clas %>%
select(Text, `Predicted Labels`, `True Labels`, run, new_topic, MISS_binary)
topics_clas2 %>%
group_by(new_topic) %>%
summarise("" = mean(MISS_binary))
topics_clas2 %>%
group_by(new_topic) %>%
summarise("" = mean(MISS_binary))
View(topics_clas2)
topics_clas2 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
e1 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report1.csv")
e2 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report2.csv")
e3 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report3.csv")
e4 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report4.csv")
e5 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report5.csv")
e6 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report6.csv")
e7 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report7.csv")
e8 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report8.csv")
e9 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report9.csv")
e10 <- read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/vestinn:ScandiBERT/df_classification_report10.csv")
# adding epoch pointer column
e1['run'] <- 1
e2['run'] <- 2
e3['run'] <- 3
e4['run'] <- 4
e5['run'] <- 5
e6['run'] <- 6
e7['run'] <- 7
e8['run'] <- 8
e9['run'] <- 9
e10['run'] <- 10
classification_report_vestINN <- rbind(e1,e2,e3,e4,e5,e6,e7,e8,e9,e10)
classification_report_vestINN['MISS_binary'] <- ifelse(classification_report_vestINN$Misclassification == "TRUE", 1, 0)
topic_clas3 <- merge(classification_report_vestINN, topics2, by = "Text", all.x = T, all.y = F)
topics_clas3 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
topic_clas3 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
metric_jonf_per_topic <- topics_clas2 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
metric_jonf_per_topic
metric_vestINN_per_topic
metric_jonf_per_topic
metric_vestINN_per_topic-metric_jonf_per_topic
metric_jonf_per_topic-metric_vestINN_per_topic
metric_vestINN_per_topic
metric_jonf_per_topic
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
message = FALSE)
library(tidyverse)
library(here)
# For text mining:
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!
ipcc_path <- here("data","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
setwd("~/Desktop/CogSci/Cultural data science/AU606111_Wibe_Jorgen/Other/SentimentAnalysis")
ipcc_path <- here("data","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
ipcc_path <- here("data","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
library(tidyverse)
library(here)
# For text mining:
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
ipcc_text <- pdf_text(ipcc_path)
ipcc_path <- here("data/","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
ipcc_path <- here("data","/","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
ipcc_path <- here("data","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
ipcc_path <- here("/Users/wibe/Desktop/CogSci/Cultural data science/AU606111_Wibe_Jorgen/Other/SentimentAnalysis/data/ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary))
ipcc_p9 <- ipcc_text[9]
ipcc_p9
ipcc_df <- data.frame(ipcc_text) %>%
mutate(text_full = str_split(ipcc_text, pattern = '\\n')) %>%
unnest(text_full) %>%
mutate(text_full = str_trim(text_full))
# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.
# Although, this time round, it is working for me with \n alone. Wonders never cease.
# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html
ipcc_tokens <- ipcc_df %>%
unnest_tokens(word, text_full)
# See how this differs from `ipcc_df`
# Each word has its own row!
ipcc_wc <- ipcc_tokens %>%
count(word) %>%
arrange(-n)
ipcc_wc
ipcc_stop <- ipcc_tokens %>%
anti_join(stop_words) %>%
select(-ipcc_text)
ipcc_swc <- ipcc_stop %>%
count(word) %>%
arrange(-n)
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed
ipcc_no_numeric <- ipcc_stop %>%
filter(is.na(as.numeric(word)))
# There are almost 2000 unique words
length(unique(ipcc_no_numeric$word))
# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
ipcc_top100 <- ipcc_no_numeric %>%
count(word) %>%
arrange(-n) %>%
head(100)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
message = FALSE)
library(tidyverse)
library(here)
# For text mining:
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!
# Chunk 2: get-document
ipcc_path <- here("/Users/wibe/Desktop/CogSci/Cultural data science/AU606111_Wibe_Jorgen/Other/SentimentAnalysis/data/ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
# Chunk 3: single-page
ipcc_p9 <- ipcc_text[9]
ipcc_p9
# Chunk 4: split-lines
ipcc_df <- data.frame(ipcc_text) %>%
mutate(text_full = str_split(ipcc_text, pattern = '\\n')) %>%
unnest(text_full) %>%
mutate(text_full = str_trim(text_full))
# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.
# Although, this time round, it is working for me with \n alone. Wonders never cease.
# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html
# Chunk 5: tokenize
ipcc_tokens <- ipcc_df %>%
unnest_tokens(word, text_full)
# See how this differs from `ipcc_df`
# Each word has its own row!
ipcc_wc <- ipcc_tokens %>%
count(word) %>%
arrange(-n)
ipcc_wc
ipcc_stop <- ipcc_tokens %>%
anti_join(stop_words) %>%
select(-ipcc_text)
ipcc_swc <- ipcc_stop %>%
count(word) %>%
arrange(-n)
ipcc_swc <- ipcc_stop %>%
count(word) %>%
arrange(-n)
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed
ipcc_no_numeric <- ipcc_stop %>%
filter(is.na(as.numeric(word)))
# There are almost 2000 unique words
length(unique(ipcc_no_numeric$word))
# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
ipcc_top100 <- ipcc_no_numeric %>%
count(word) %>%
arrange(-n) %>%
head(100)
ipcc_cloud <- ggplot(data = ipcc_top100, aes(label = word)) +
geom_text_wordcloud() +
theme_minimal()
ipcc_cloud
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "guitar") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "star") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "penis") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "square") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
geom_text_wordcloud_area(aes(color = n), shape = "pentagon") +
scale_size_area(max_size = 12) +
scale_color_gradientn(colors = c("darkgreen","blue","red")) +
theme_minimal()
mean(topic_clas3$MISS_binary)
test <- mean(topic_clas3$MISS_binary)
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary)-test)
metric_vestINN_per_topic
test1 <- mean(topics_clas2$MISS_binary)
metric_jonf_per_topic <- topics_clas2 %>%
group_by(new_topic) %>%
summarise("2" = mean(MISS_binary)-test1)
metric_vestINN_per_topic
metric_jonf_per_topic
#difference
metric_jonf_per_topic-metric_vestINN_per_topic
head(topic_clas3)
View(topic_clas3)
#difference
metric_jonf_per_topic-metric_vestINN_per_topic
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary)-test)
metric_vestINN_per_topic
metric_jonf_per_topic <- topics_clas2 %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary)-test1)
#difference
vizz <- metric_jonf_per_topic-metric_vestINN_per_topic
#difference
metric_jonf_per_topic-metric_vestINN_per_topic
#difference
vizz <- metric_jonf_per_topic-metric_vestINN_per_topic
ggplot(vizz, aes(x = "2", new_topic)),
geom_point()
ggplot(vizz, aes(x = "2", new_topic)) +
geom_point()
ggplot(vizz, aes(x = new_topic, 2)) +
geom_point()
ggplot(vizz, aes(as.factor(new_topic), 2)) +
geom_point()
View(vizz)
vizz %>%
group_by(new_topic, run) %>%
ggplot(aes(x=2))+
geom_histogram()
vizz %>%
group_by(new_topic, run) %>%
rename(rates = 2) %>%
ggplot(aes(x="2"))+
geom_histogram()
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x="2"))+
geom_histogram()
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x=rates))+
geom_histogram()
vizz
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x=rates))+
geom_histogram()
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x=rates))+
geom_histogram()+
theme_bw()
names(vizz)
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x=new_topic, y = rates))+
geom_point()+
theme_bw()
vizz
metric_jonf_per_topic
View(metric_jonf_per_topic)
vizz
metric_jonf_per_topic
#difference
vizz <- metric_jonf_per_topic$`2`-metric_vestINN_per_topic`2`
#difference
vizz <- metric_jonf_per_topic$`2`-metric_vestINN_per_topic$`2`
vizz %>%
group_by(new_topic, run) %>%
rename(rates = `2`) %>%
ggplot(aes(x=new_topic, y = rates))+
geom_point()+
theme_bw()
vizz
#difference
vizz <- metric_jonf_per_topic %>%
mutate(diff = metric_jonf_per_topic$`2`-metric_vestINN_per_topic$`2`)
vizz
#difference
vizz <- metric_jonf_per_topic %>%
mutate(diff = metric_jonf_per_topic$`2`-metric_vestINN_per_topic$`2`)
#difference
vizz <- metric_jonf_per_topic %>%
mutate(diff = `2`-metric_vestINN_per_topic$`2`)
#difference
vizz <- metric_jonf_per_topic %>%
mutate("diff" = `2`-metric_vestINN_per_topic$`2`)
View(metric_vestINN_per_topic)
metric_jonf_per_topic$2 - metric_vestINN_per_topic$2
metric_jonf_per_topic$`2` - metric_vestINN_per_topic$`2`
temp <- metric_jonf_per_topic$`2` - metric_vestINN_per_topic$`2`
temp_df <- metric_jonf_per_topic %>%
select(new_topic, run)
temp_df <- cbind(temp_df, temp)
View(temp_df)
temp_df <- temp_df %>%
rename("sub_group_error" = `..3`)
temp_df <- temp_df %>%
rename("sub_group_error" = `...3`)
temp_df %>%
group_by(new_topic, run) %>%
ggplot(aes(x=new_topic, y = sub_group_error))+
geom_point()+
theme_bw()
rbind(metric_jonf_per_topic, metric_vestINN_per_topic)
metric_jonf_per_topic$model <- "jonf"
metric_vestINN_per_topic$model <- "vestInn"
rbind(metric_jonf_per_topic, metric_vestINN_per_topic)
temp_df <- rbind(metric_jonf_per_topic, metric_vestINN_per_topic)
temp_df <- temp_df %>%
rename("sub_group_error" = `2`)
temp_df %>%
ggplot(aes(x=new_topic, y = sub_group_error, colour = model))+
geom_point()+
theme_bw()
temp_df %>%
ggplot(aes(x=new_topic, y = sub_group_error, colour = model))+
geom_point()+
geom_jitter() +
theme_bw()
temp_df$new_topic <- as.character(temp_df$new_topic)
temp_df %>%
ggplot(aes(x=new_topic, y = sub_group_error, colour = model))+
geom_point()+
geom_jitter() +
theme_bw()
temp_df %>%
ggplot(aes(x=as.factor(new_topic), y = sub_group_error, colour = model))+
geom_point()+
geom_jitter() +
theme_bw()
temp <- metric_jonf_per_topic$`2` - metric_vestINN_per_topic$`2`
metric_jonf_per_topic$model <- "jonf"
metric_vestINN_per_topic$model <- "vestInn"
temp_df <- rbind(metric_jonf_per_topic, metric_vestINN_per_topic)
temp_df <- temp_df %>%
rename("sub_group_error" = `2`)
temp_df$new_topic <- as.factor(temp_df$new_topic)
temp_df %>%
ggplot(aes(x=new_topic, y = sub_group_error, colour = model))+
geom_point()+
geom_jitter() +
theme_bw()
temp_df %>%
ggplot(aes(x=new_topic, y = sub_group_error, colour = model))+
geom_point()+
theme_bw()
# Error rate pr run
classification_report_jondf %>%
group_by(run) %>%
summarise("over_all_error_rate" = mean(MISS_binary))
#Overall error rate
classification_report_jondf %>%
summarise("over_all_error_rate" = mean(MISS_binary))
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary))
topic_clas3 %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary))
metric_vestINN_per_topic
View(metric_vestINN_per_topic)
test <- topic_clas3 %>%
group_by(run) %>%
mean(MISS_binary)
test <- topic_clas3 %>%
group_by(run) %>%
summarise("mean" = mean(MISS_binary))
View(test)
metric_vestINN_per_topic <- topic_clas3 %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary)-test)
merge(x = topic_clas3, y = test, by = "run", all.x = T, all.y = F)
topic_clas3_temp <- merge(x = topic_clas3, y = test, by = "run", all.x = T, all.y = F)
View(topic_clas3_temp)
metric_vestINN_per_topic <- topic_clas3_temp %>%
group_by(new_topic, run) %>%
summarise("2" = mean(MISS_binary)-mean(mean))
metric_vestINN_per_topic
View(metric_vestINN_per_topic)
topic_clas3_temp <- merge(x = topic_clas3, y = test, by = "run", all.x = T, all.y = F)
tbale(topic_clas3_temp$mean)
table(topic_clas3_temp$mean)
# Error rate pr run
classification_report_jondf %>%
group_by(run) %>%
summarise("over_all_error_rate" = mean(MISS_binary))
setwd("~/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/scripts")
read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/jonfd_electra-small-nordic/df_classification_report5.csv")
names(read_csv("/Users/wibe/Desktop/CogSci/Bachelor thesis/bachelor_thesis/analysis_R/data/jonfd_electra-small-nordic/df_classification_report5.csv"))
read_plus <- function(filename) {
read_csv(filename) %>%
mutate(filename = filename)
}
tbl_with_sources <-
list.files(path = "data/jonfd_electra-small-nordic", pattern = "*.csv",
full.names = TRUE) %>%
map_df(~read_plus(.))
pacman::p_load(tidyverse)
pacman::p_load(tidyverse)
read_plus <- function(filename) {
read_csv(filename) %>%
mutate(filename = filename)
}
tbl_with_sources <-
list.files(path = "data/jonfd_electra-small-nordic", pattern = "*.csv",
full.names = TRUE) %>%
map_df(~read_plus(.))
