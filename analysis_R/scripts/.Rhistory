# flax
classf_report_flax <- list.files(path = "../data/flax", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_flax['MISS_binary'] <- ifelse(classf_report_flax$Misclassification == "TRUE", 1, 0)
classf_report_flax$model <- 7
# danish-bert-botxo
classf_report_danish_bert_botxo <- list.files(path = "../data/danish-bert-botxo", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_danish_bert_botxo['MISS_binary'] <- ifelse(classf_report_danish_bert_botxo$Misclassification == "TRUE", 1, 0)
classf_report_danish_bert_botxo$model <- 8
# mdeberta-v3-base
classf_report_mdeberta_v3_base <- list.files(path = "../data/mdeberta-v3-base", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_mdeberta_v3_base['MISS_binary'] <- ifelse(classf_report_mdeberta_v3_base$Misclassification == "TRUE", 1, 0)
classf_report_mdeberta_v3_base$model <- 9
########## LOADING TOPICS ###############
topics <- read_csv("../data/BerTopic/sub_groups_14112022.csv") %>%
rename(Text = original_tweet)
# combining models
modelscombined <- rbind(classf_report_jonf, classf_report_vestin, classf_report_aelectra, classf_report_XLM_base, classf_report_XLM_large, classf_report_twitter_xlm_roberta, classf_report_flax,classf_report_danish_bert_botxo, classf_report_mdeberta_v3_base)
names(classf_report_aelectra)
names(classf_report_flax)
# merging with topics
all_models_w_topics <- merge(modelscombined,topics, by = "Text", all.x = T, all.y = F) %>%
rename(run = filename) %>%
select(Text, `Predicted Labels`, `True Labels`, new_topic, MISS_binary, run)
all_models_w_topics <- all_models_w_topics[complete.cases(all_models_w_topics), ] # removing rows with NA's in topic (50 rows of which 10 were in Arabic)
View(all_models_w_topics)
table(is.na(all_models_w_topics))
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse,devtools,rethinking)
######## LOADING CLASSIFICATION REPORT #################
# Function for loading multiple files
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(filename = flnm)
}
# jonfd_electra-small-nordic
classf_report_jonf <- list.files(path = "../data/jonfd_electra-small-nordic", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_jonf['MISS_binary'] <- ifelse(classf_report_jonf$Misclassification == "TRUE", 1, 0)
classf_report_jonf$model <- 1
# vestinn/ScandiBERT
classf_report_vestin <- list.files(path = "../data/vestinn_ScandiBERT", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_vestin['MISS_binary'] <- ifelse(classf_report_vestin$Misclassification == "TRUE", 1, 0)
classf_report_vestin$model <- 2
# aelectra
classf_report_aelectra <- list.files(path = "../data/aelectra", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_aelectra['MISS_binary'] <- ifelse(classf_report_aelectra$Misclassification == "TRUE", 1, 0)
classf_report_aelectra$model <- 3
# xlm-roberta-base
classf_report_XLM_base <- list.files(path = "../data/xlm-roberta-base", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_XLM_base['MISS_binary'] <- ifelse(classf_report_XLM_base$Misclassification == "TRUE", 1, 0)
classf_report_XLM_base$model <- 4
# xlm_roberta_large
classf_report_XLM_large <- list.files(path = "../data/xlm_roberta_large", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_XLM_large['MISS_binary'] <- ifelse(classf_report_XLM_large$Misclassification == "TRUE", 1, 0)
classf_report_XLM_large$model <- 5
# twitter-xlm-roberta
classf_report_twitter_xlm_roberta <- list.files(path = "../data/twitter-xlm-roberta", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_twitter_xlm_roberta['MISS_binary'] <- ifelse(classf_report_twitter_xlm_roberta$Misclassification == "TRUE", 1, 0)
classf_report_twitter_xlm_roberta$model <- 6
# flax
classf_report_flax <- list.files(path = "../data/flax", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_flax['MISS_binary'] <- ifelse(classf_report_flax$Misclassification == "TRUE", 1, 0)
classf_report_flax$model <- 7
# danish-bert-botxo
classf_report_danish_bert_botxo <- list.files(path = "../data/danish-bert-botxo", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_danish_bert_botxo['MISS_binary'] <- ifelse(classf_report_danish_bert_botxo$Misclassification == "TRUE", 1, 0)
classf_report_danish_bert_botxo$model <- 8
# mdeberta-v3-base
classf_report_mdeberta_v3_base <- list.files(path = "../data/mdeberta-v3-base", pattern = "*.csv",
full.names = T) %>%
map_df(~read_plus(.))
classf_report_mdeberta_v3_base['MISS_binary'] <- ifelse(classf_report_mdeberta_v3_base$Misclassification == "TRUE", 1, 0)
classf_report_mdeberta_v3_base$model <- 9
##### COMBINING MODELS #####
modelscombined <- rbind(classf_report_jonf, classf_report_vestin, classf_report_aelectra, classf_report_XLM_base, classf_report_XLM_large, classf_report_twitter_xlm_roberta, classf_report_flax,classf_report_danish_bert_botxo, classf_report_mdeberta_v3_base)
########## LOADING TOPICS ###############
topics <- read_csv("../data/BerTopic/sub_groups_14112022.csv") %>%
rename(Text = original_tweet)
##### MERGING MODELS WITH TOPICS
all_models_w_topics <- merge(modelscombined,topics, by = "Text", all.x = T, all.y = F) %>%
rename(run = filename) %>%
select(Text, `Predicted Labels`, `True Labels`, new_topic, MISS_binary, run, model)
all_models_w_topics <- all_models_w_topics[complete.cases(all_models_w_topics), ] # removing rows with NA's in topic (50 rows of which 10 were in Arabic)
error_rate_per_run_per_topic <- all_models_w_topics %>%
group_by(new_topic, run, model_) %>%
summarise("error_rate_per_run_per_topic" = mean(MISS_binary))
error_rate_per_run_per_topic <- all_models_w_topics %>%
group_by(new_topic, run, model) %>%
summarise("error_rate_per_run_per_topic" = mean(MISS_binary))
error_rate_per_run_across_topics <- all_models_w_topics %>%
group_by(run, model) %>%
summarise("error_rate_per_run_across_topics" = mean(MISS_binary))
tress <- merge(error_rate_per_run_per_topic, error_rate_per_run_across_topics, by = "run", all.x = T, all.y = F)
quatro <- tress %>%
mutate(diff = error_rate_per_run_per_topic - error_rate_per_run_across_topics)
singo <- quatro %>%
#filter(new_topic==3) %>%
rename(ourmodel = `model_.x`) %>%
rename(difference = diff) %>%
select(run, new_topic, ourmodel,difference)
singo <- quatro %>%
#filter(new_topic==3) %>%
rename(ourmodel = model) %>%
rename(difference = diff) %>%
select(run, new_topic, ourmodel,difference)
View(tress)
singo <- quatro %>%
#filter(new_topic==3) %>%
rename(ourmodel = `model.x`) %>%
rename(difference = diff) %>%
select(run, new_topic, ourmodel,difference)
final <-  singo %>%
select(ourmodel,difference, new_topic)
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse,devtools,rethinking)
### COMPARING THE MEANS WITHIN MODEL
pacman::p_load(tidybayes.rethinking)
func_TEMP <- function(topic){
func_df <- final %>%
filter(new_topic == topic)
func_model <- ulam(
alist(
difference ~ dnorm(mu, sigma),
mu <- a[ourmodel],
a[ourmodel] ~ dnorm(0.40, 0.15) ,
sigma ~ dexp(2)
), data = func_df, chains = 4, cores = 4)
func_precis <- precis(func_model, depth = 2)
return(list(func_precis, func_model))
}
func_TEMP <- function(topic){
func_df <- final %>%
filter(new_topic == topic)
func_model <- ulam(
alist(
difference ~ dnorm(mu, sigma),
mu <- a[ourmodel],
a[ourmodel] ~ dnorm(0.40, 0.15) ,
sigma ~ dexp(2)
), data = func_df, chains = 4, cores = 4)
func_precis <- precis(func_model, depth = 2)
return(list(func_precis, func_model))
}
output_list <- func_TEMP(3)
output_list <- func_TEMP(3)
temp_df <-  final %>%
filter(new_topic == 3)
temp_model <- ulam(
alist(
difference ~ dnorm(mu, sigma),
mu <- a[ourmodel],
a[ourmodel] ~ dnorm(0.40, 0.15) ,
sigma ~ dexp(2)
), data = temp_df, chains = 4, cores = 4)
output_list
post <- extract.samples(temp_model)
post$diff_12 <- post$a[,1] - post$a[,2]
post$diff_13 <- post$a[,1] - post$a[,3]
post$diff_14 <- post$a[,1] - post$a[,4]
post$diff_15 <- post$a[,1] - post$a[,5]
post$diff_23 <- post$a[,2] - post$a[,3]
post$diff_24 <- post$a[,2] - post$a[,4]
post$diff_25 <- post$a[,2] - post$a[,5]
post$diff_34 <- post$a[,3] - post$a[,4]
post$diff_35 <- post$a[,3] - post$a[,5]
post$diff_45 <- post$a[,4] - post$a[,5]
precis( post , depth=2 )
plot( precis( post , depth=2))
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pacman::p_load(rvest,dplyr,tidyr,stringr,janitor)
url <- "https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene"
# scrape the website
url_html <- read_html(url)
whole_table <- url_html %>%
html_nodes("table") %>%
html_table()
str(whole_table) #turns out to be a list
head(whole_table)
install.packages("RSelenium")
?RSelenium
??RSelenium
whole_table <- url_html %>%
html_nodes("rich-text") %>%
html_table()
str(whole_table) #turns out to be a list
head(whole_table)
?html_nodes()
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pacman::p_load(rvest,dplyr,tidyr,stringr,janitor)
url <- "https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene"
# scrape the website
url_html <- read_html(url)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pacman::p_load(rvest,dplyr,tidyr,stringr,janitor)
url <- "https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene"
# scrape the website
url_html <- read_html(url)
?html_nodes()
library(dplyr)
library(magrittr)
library(stringr)
library(rvest)
# load packages
# First, we need to load the package we'll use for most of our scraping.
# It is called *rvest*.
pacman::p_load(dplyr, magrittr,stringr,rvest)
url <- "https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene"
url
searchresults <- read_html(url)
searchresults
# How does the code of a website look like?
browseURL("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
# How does the code of a website look like?
browseURL("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
?extract2
page <- read_html("https://en.wikipedia.org/wiki/Eiffel_Tower_replicas_and_derivatives")
table <- html_table(page, fill = T)  %>%
extract2(1)
table
# Then count which country has the most Eiffel tower replicas. You can use the tally() command
table %>%
group_by(Country) %>%
tally() %>%
slice_max(n)
table %>%
group_by(Country) %>%
tally() %>%
arrange(desc(n))
# ggplot
library(ggplot2)
ggplot(table,aes(x=Country))+
geom_histogram(stat="count")+
theme(axis.text.x = element_text(angle = 90))
# Let's try it with a speech by the Danish prime minister.
browseURL("https://english.stm.dk/the-prime-minister/speeches/prime-minister-mette-frederiksen-s-new-years-speech-on-the-1st-of-january-2022/")
library(pacman)
pacman::p_load(rvest,dplyr,tidyr,stringr,janitor)
url <- "https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene"
# scrape the website
url_html <- read_html(url)
whole_table <- url_html %>%
html_nodes("..rich-text div a") %>%
html_table()
whole_table <- url_html %>%
html_nodes("//*[contains(concat( " ", @class, " " ), concat( " ", "rich-text", " " ))]//div//aa") %>%
html_table()
whole_table <- url_html %>%
html_nodes(".rich-text div a") %>%
html_table()
str(whole_table) #turns out to be a list
head(whole_table)
View(whole_table)
whole_table
str(whole_table) #turns out to be a list
whole_table <- url_html %>%
html_nodes(".plh-main") %>%
html_table()
str(whole_table) #turns out to be a list
whole_table <- url_html %>%
html_nodes(".rich-text") %>%
html_table()
str(whole_table) #turns out to be a list
browseURL("https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene")
searchresults <- read_html("https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene")
searchresults %>% html_nodes("*") %>% html_attr("href")
class(searchresults)
class(searchresults)
searchresults %>% html_nodes(".views-field-title a") %>% html_attr("href")
test <- searchresults[complete.cases(searchresults), ] # removing rows with NA's in topic (50 rows of which 10 were in Arabic)
urls <- searchresults %>% html_nodes(".views-field-title a") %>% html_attr("href")
urls
class(searchresults)
searchresults %>% html_nodes("*") %>% html_attr("href")
searchresults %>% html_nodes("<a") %>% html_attr("href")
searchresults %>% html_nodes("a") %>% html_attr("href")
class(searchresults)
list(searchresults)
test <- list(searchresults)
searchresults %>% html_nodes("a") %>% html_attr("href")
class(searchresults)
searchresults %>% html_nodes("a") %>% html_attr("href")
test <- searchresults %>% html_nodes("a") %>% html_attr("href")
list(searchresults)
searchresults %>% html_nodes("a") %>% html_attr("href")
list(test)
test2 <- test[108:713]
test2
urls <- test2
urls <- paste0("https://hojskolesangbogen.dk/",urls)
urls
write_csv(urls, )
?write_csv
?write_csv2
write_csv(urls, "/Users/wibe/Desktop/urls.csv")
write_csv("/Users/wibe/Desktop/urls.csv",urls)
write_csv2(urls, "/Users/wibe/Desktop/urls.csv")
write_csv2(urls, "urls.csv")
test <- as.data.frame(urls)
write_csv2(test, "urls.csv")
write_csv2(test, "/Users/wibe/Desktop/urls.csv")
songtexts <- read_html(urls[1]) %>%
html_nodes(".field__item") %>%
html_text()
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes(".text") %>%
html_text()
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes(".rich-text") %>%
html_text()
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes("div.rich-text") %>%
html_text()
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes(".field__item") %>%
html_text()
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes(".field_item") %>%
html_text()
songtexts
songtexts
songtexts <- read_html(urls[1]) %>%
html_nodes("div.rich-text") %>%
html_text()
songtexts
speech_df <- NULL
for (i in 1:length(urls)){
text <- read_html(urls[i]) %>%
html_nodes("div.rich-text") %>%
html_text()
df_temp <- data.frame(speech = text)
speech_df <- rbind(speech_df)
}
for (i in 1:length(urls)){
text <- read_html(urls[i]) %>%
html_nodes("div.rich-text") %>%
html_text()
df_temp <- data.frame(speech = text)
}
View(df_temp)
df_temp
pacman::p_load(geniusr)
pacman::p_load(geniusr)
install.packages("geniusr")
library(geniusr)
get_song()
get_song(22)
get_song(song_id = 22)
View(df_temp)
urls
songtexts <- read_html(urls[1]) %>%
html_nodes("div.rich-text") %>%
html_text()
songtexts
songtexts[2]
songtexts[2]
urls
read_html(urls[1])
pacman::p_load(RSelenium,dplyr, magrittr,stringr,rvest)
pacman::p_load(RSelenium,dplyr, magrittr,stringr,rvest)
searchresults <- read_html("https://hojskolesangbogen.dk/om-sangbogen/historie-om-bogen/19-udgave/liste-over-alle-sangene")
searchresults %>% html_nodes("*") %>% html_attr("href")
searchresults %>% html_nodes("a") %>% html_attr("href")
urls <- searchresults %>% html_nodes(".views-field-title a") %>% html_attr("href")
urls
searchresults %>% html_nodes(".views-field-title a") %>% html_attr("href")
searchresults %>% html_nodes("a") %>% html_attr("href")
test <- searchresults %>% html_nodes("a") %>% html_attr("href")
test2 <- test[108:713]
test2
class(test2)
test2 <- list(test[108:713])
test2
test2 <- test[108:713]
test2
urls1 <- searchresults %>% html_nodes("a") %>% html_attr("href")
urls2 <- urls1[108:713]
urls3 <- paste0("https://hojskolesangbogen.dk/",urls2)
urls3
urls3 <- paste0("https://hojskolesangbogen.dk",urls2)
urls3
songtexts <- read_html(urls[1]) %>%
html_nodes("div.rich-text") %>%
html_text()
read_html(urls[1]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[1]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[2]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[3]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[42]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[456]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[399]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[399]) %>% html_nodes("div.rich-text") %>% html_text() %>% [2]
read_html(urls3[399]) %>% html_nodes("div.rich-text") %>% html_text()[2] %>%
```
text <- read_html(urls3[399]) %>% html_nodes("div.rich-text") %>% html_text()
text[2]
for (i in 1:length(urls3)) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
text2 <- vector("list", length = length(urls3)))
text2 <- vector("list", length = length(urls3))
for (i in 1:length(urls3)) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
for (i in 1:3) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
text2 <- vector("list", length = 3)
for (i in 1:3) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
View(text2)
text3
text2
text2 <- vector("list", length = length(urls3))
for (i in 1:length(urls3)) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
text2
as.tibble(text2)
text2
read_html(urls3[1]) %>% html_nodes("div.rich-text") %>% html_text()
read_html(urls3[500]) %>% html_nodes("div.rich-text") %>% html_text()
text2
text2
warnings()
last_wanings(n = 5)
last_warnings(n=5)
last_warnings(n = NULL)
last_messages(n = 5)
last.warning()
last_warnings(5)
last_error(n=5)
last_error(n=15)
warnings(...)
last_warnings(5)
traceback()
.traceback()
traceback()
urls3
#removing 435 and 437
urls3[-c(435,437)]
urls3
#removing 435 and 437
urls3 <- urls3[-c(435,437)]
view(as.data.frame(urls3))
# Getting text for each song
text2 <- vector("list", length = length(urls3))
#text2 <- as.tibble()
for (i in 1:length(urls3)) {
text <- read_html(urls3[i]) %>% html_nodes("div.rich-text") %>% html_text()
text2[i] <- text[2]
}
text2
write_csv(text2, "/Users/wibe/Desktop/sangtekster.csv")
write_csv2(text2, "/Users/wibe/Desktop/sangtekster.csv")
write_csv2(as.data.frame(text2), "/Users/wibe/Desktop/sangtekster.csv")
write_csv(as.data.frame(text2), "/Users/wibe/Desktop/sangtekster.csv")
as.tibble(text2)
write_csv(list(text2), "/Users/wibe/Desktop/sangtekster.csv")
as_tibble(text2)
tibble(text2)
write_csv(tibble(text2), "/Users/wibe/Desktop/sangtekster.csv")
text3 <- tibble(text2)
write_csv(text3, "/Users/wibe/Desktop/sangtekster.csv")
View(text3)
write_csv(text3, "/Users/wibe/Desktop/sangtekster.csv")
View(text3)
text3
head(text3)
head(text3$text2)
head(text3$text2)
class(text2)
tibble(text = text2, ID = NA)
text2
rbindlist(text2)
library(Matrix)
library(data.table)
rbindlist(text2)
