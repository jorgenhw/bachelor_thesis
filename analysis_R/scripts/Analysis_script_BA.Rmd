---
title: "Analysis_script_BA"
subtitle: "Bachelor thesis"
author: "Jørgen Højlund Wibe"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse)
```

# Loading data
```{r}
# Classification report (= classf_report_jonf)
read_plus <- function(flnm) {
    read_csv(flnm) %>% 
        mutate(filename = flnm)
}
classf_report_jonf <- list.files(path = "../data/jonfd_electra-small-nordic", pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_plus(.))

# Topics (= topics)
topics <- read_csv("../data/BerTopic/sub_groups_14112022.csv") %>% 
  rename(Text = original_tweet) 
```

# Calculating error rates
```{r}
# Creating a numeric variable for miss/true
classf_report_jonf['MISS_binary'] <- ifelse(classf_report_jonf$Misclassification == "TRUE", 1, 0)
```

### Overall error rate pr run
```{r}
classf_report_jonf %>% 
  group_by(filename) %>% 
  summarise("error_rate_per_run" = mean(MISS_binary))
```

### Overall error rate over all runs
```{r}
#Overall error rate
classf_report_jonf %>% 
  summarise("over_all_error_rate" = mean(MISS_binary))
```

### Error rates per topic across runs
```{r}
# Merging topics with classification report and minimizing df
topic_classf <- merge(classf_report_jonf,topics, by = "Text", all.x = T, all.y = F) %>% 
  rename(run = filename) %>% 
  select(Text, `Predicted Labels`, `True Labels`, new_topic, MISS_binary, run)
topic_classf <- topic_classf[complete.cases(topic_classf), ] # removing rows with NA's in topic (50 rows of which 10 were in Arabic)

# Seeing how many topics are in each category after merging
topic_classf %>% 
  ggplot(aes(new_topic, fill = run)) +
  geom_bar()+
  labs(title = "Number of entries per topic after merging with classification report", subtitle = "NB: Divide y-axis by 10")

# Error rates per topic across runs
topic_classf %>% 
  group_by(new_topic, run) %>% 
  summarise("error_rate_per_run" = mean(MISS_binary)) %>% 
  summarise("error_rate_per_topic_across_runs" = mean(error_rate_per_run))
```

### Error rates per topic per run subtracted from overall error rate per topic across all runs
```{r}
uno <- topic_classf %>% 
  group_by(new_topic, run) %>% 
  summarise("error_rate_per_run_per_topic" = mean(MISS_binary))

dos <- topic_classf %>% 
  group_by(run) %>% 
  summarise("error_rate_per_run_across_topics" = mean(MISS_binary))

tress <- merge(uno, dos, by ="run", all.x = T, all.y = F)

quatro <- tress %>% 
  mutate(diff = error_rate_per_run_per_topic - error_rate_per_run_across_topics)


topic_classf %>% 
  group_by(new_topic, run) %>% 
  summarise("error_rate_per_run" = mean(MISS_binary)) %>% 
  ungroup(new_topic, run) %>% 
  group_by(run) %>% 
  summarise("error_rate_per_topic_across_runs" = mean(error_rate_per_run))
  #summarise("error_rates_per_topic_relative_to_overall_error_rate_pr_topic" = error_rate_per_topic_across_runs - mean(topic_classf$MISS_binary))
```

### Error rates per run for each topic
```{r}
topic_classf %>% 
  group_by(run, new_topic) %>% 
  summarise("error_rate_per_run_across_all_topics" = mean(MISS_binary)) 
```

# Visualizations
```{r}
topic_classf %>% 
  group_by(new_topic) %>% 
  summarise("average_error" = mean(MISS_binary)) %>% 
  ggplot(aes(x = new_topic, y = average_error))+
  geom_point() +
  ggtitle("Box Plot")
```




# bays


```{r}
singo <- quatro %>% 
  filter(new_topic==3)

singo$model_ = 1

singosingo <- quatroquatro %>% 
  filter(new_topic == 3)

singosingo$model_ = 2

twomodels <- rbind(singo, singosingo)

twomodels_uno <- twomodels %>% 
  select(diff, model_)

#write.csv(twomodels_uno,"twomodels_uno.csv")

#twomodels_uno$model <- as.factor(twomodels_uno$model)

library(rethinking)

m1 <- ulam(
    alist(
        diff ~ dnorm(mu, sigma),
        mu <- a[model_],
        a[model_] ~ dnorm(0.40, 0.15) ,
        sigma ~ dexp(2) 
    ), data = twomodels_uno, chains = 4, cores = 7)



precis(m1, depth =2)
plot(coeftab(m1),by.model=TRUE)
```


```{r}



```







